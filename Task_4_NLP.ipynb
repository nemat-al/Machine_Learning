{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nemat_Allah_Aloush_J41332c_MLT_2022_Task_4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcd9OPRq21YTck9xj2fPFB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nemat-Allah-Aloush/Machine_Learning_Techinques/blob/main/Task_4_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nemat_Allah_Aloush_J41332c_MLT_2022_Task_4\n",
        "* Name: Nemat Allah Aloush\n",
        "* ISU group: J41332c\n",
        "* ISU number: 336092"
      ],
      "metadata": {
        "id": "-SlYubKN58wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing packages"
      ],
      "metadata": {
        "id": "S68tAz-fP6mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# import all the resources for Natural Language Processing with Python\n",
        "nltk.download(\"book\")\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "kvCmSNtgjhW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Reading data file"
      ],
      "metadata": {
        "id": "JT8j0rnaP-u3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZSZJAPLhzHd",
        "outputId": "a601734e-12b3-4950-a0bd-bf097676591d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Mounting to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the file (Alice in wonder land story)\n",
        "filepath = nltk.data.find('/content/drive/My Drive/Machine Learning Techniques 2022/Alice_in_wonder_land.txt')\n",
        "textfile= open(filepath, 'r').read()"
      ],
      "metadata": {
        "id": "acYGI2l7oFeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The file contains a paragraph before the story and onthore one after the ending of the story. \n",
        "# In the following I am splitting the content of the file to delete the latter paragraph after the ending of the story.  \n",
        "splitted = textfile.split(\"THE END \", 1) \n",
        "# In the following I am splitting the result to delete the first paragraph that was written before the story.  \n",
        "story = splitted[0].split(\"[Illustration]\",1)\n",
        "story = story[1] # story variable contains the story and it is the variable to analyze in the further steps."
      ],
      "metadata": {
        "id": "4KHqQybAl8pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Cleaning Text Data"
      ],
      "metadata": {
        "id": "4_j_ttsrQLwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words\n",
        "tokens = word_tokenize(story)\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "# remove punctuation from each word\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "# lemmatizarion\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "lemmas = [wn.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "v2RLU4hnMaki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Top 10 most important (in terms of TF-IDF metric) words from each chapter in the text and naming each chapter according to the identified tokens"
      ],
      "metadata": {
        "id": "uhr0j1wlSxNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Splitting the lemmas array to individual chapters"
      ],
      "metadata": {
        "id": "C2Qk-A17pZGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the indices of occurences of the word 'chapter' to select the start and end of each chapter\n",
        "occurences =[i for i,x in enumerate(lemmas) if x=='chapter']\n",
        "occurences.append(len(lemmas))  # the last word (index) in the 'lemmas' variable is the end of the last chapter "
      ],
      "metadata": {
        "id": "TtGYIfGafofl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices=[]\n",
        "#First 12 occurence of the word 'chapter' are just the outline of the story, thus we delete them\n",
        "chapter_points=occurences[12:] \n",
        "# building a list to contain for each chapter the indices for its start and ending.\n",
        "for i in range (len(chapter_points)-1):\n",
        "  indices.append((chapter_points[i],chapter_points[i+1]))\n",
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUepFv_thiFR",
        "outputId": "41a0b0bd-6432-4c92-b7f1-f33abcd78211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(59, 1011),\n",
              " (1011, 1944),\n",
              " (1944, 2736),\n",
              " (2736, 3876),\n",
              " (3876, 4837),\n",
              " (4837, 6005),\n",
              " (6005, 7057),\n",
              " (7057, 8183),\n",
              " (8183, 9238),\n",
              " (9238, 10185),\n",
              " (10185, 11038),\n",
              " (11038, 11976)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'chapters' variable contains lists of lemmas for each chapter individually \n",
        "chapters = [lemmas [s:e] for s,e in indices]"
      ],
      "metadata": {
        "id": "cyPdrbKWgywl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the word 'Alice' from each paragraph (avoiding getting 'Alice' as one of the most important words)\n",
        "chapters_without_alice = [[word for word in chapter if word != 'alice'] for chapter in chapters]"
      ],
      "metadata": {
        "id": "o-2lFAGCNUF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for each chapter, join the tokens together \n",
        "chapters_paragraphs=[' '.join(i) for i in chapters_without_alice]"
      ],
      "metadata": {
        "id": "ZLd1rrGcIbSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Calculating TF-IDF and finding top 10 words for each document"
      ],
      "metadata": {
        "id": "IJImOwatqSaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model\n",
        "tfidf = TfidfVectorizer() # Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "X_tfidf = tfidf.fit_transform(chapters_paragraphs).toarray() # Learn vocabulary and idf, return document-term matrix\n",
        "vocab = tfidf.vocabulary_  # A mapping of terms to feature indices.\n",
        "reverse_vocab = {v:k for k,v in vocab.items()}\n",
        "feature_names = tfidf.get_feature_names()\n",
        "df_tfidf = pd.DataFrame(X_tfidf, columns = feature_names)\n",
        "\n",
        "idx = X_tfidf.argsort(axis=1)   # Sorting\n",
        "tfidf_max10 = idx[:,-10:]       # Top 10\n",
        "df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10 ]"
      ],
      "metadata": {
        "id": "7AphZkAAMfWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Top 10 words for each chapter\n",
        "for (i, item) in enumerate(df_tfidf['top10'] , start=1):\n",
        "    print('Chapter', i, item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vRGD2a9TC7A",
        "outputId": "723b7d9f-7a43-46f9-d5d8-38f5294a31ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1 ['bottle', 'see', 'either', 'way', 'like', 'eat', 'key', 'door', 'bat', 'little']\n",
            "Chapter 2 ['go', 'mabel', 'foot', 'said', 'dear', 'cat', 'swam', 'little', 'pool', 'mouse']\n",
            "Chapter 3 ['dinah', 'bird', 'know', 'thimble', 'dry', 'lory', 'prize', 'dodo', 'said', 'mouse']\n",
            "Chapter 4 ['one', 'chimney', 'glove', 'fan', 'bottle', 'puppy', 'rabbit', 'window', 'little', 'bill']\n",
            "Chapter 5 ['well', 'little', 'father', 'size', 'youth', 'egg', 'pigeon', 'serpent', 'said', 'caterpillar']\n",
            "Chapter 6 ['cook', 'like', 'wow', 'duchess', 'pig', 'mad', 'baby', 'footman', 'cat', 'said']\n",
            "Chapter 7 ['clock', 'draw', 'tea', 'time', 'twinkle', 'hare', 'march', 'said', 'dormouse', 'hatter']\n",
            "Chapter 8 ['executioner', 'procession', 'five', 'cat', 'soldier', 'gardener', 'king', 'hedgehog', 'said', 'queen']\n",
            "Chapter 9 ['say', 'never', 'went', 'queen', 'moral', 'duchess', 'gryphon', 'mock', 'said', 'turtle']\n",
            "Chapter 10 ['whiting', 'beautiful', 'join', 'soup', 'dance', 'lobster', 'said', 'gryphon', 'mock', 'turtle']\n",
            "Chapter 11 ['tart', 'officer', 'juror', 'queen', 'witness', 'dormouse', 'court', 'said', 'hatter', 'king']\n",
            "Chapter 12 ['white', 'rabbit', 'slate', 'would', 'dream', 'sister', 'queen', 'jury', 'king', 'said']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason I am getting some verbs not converted to their lemmas is that the WordNetLemmatizer function from nltk library consider the default POS tag for the passed word as a noun. Thus, the correct way to call it, is by passing the POS tag of the word with the word itself, as done later in Part 4. "
      ],
      "metadata": {
        "id": "TXDUF44yvVOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Depending on the results**, Trying to give titles for each chapter:\n",
        "\n",
        "\n",
        "1. The door key.\n",
        "2. Swimming in the pool.\n",
        "3. To whom the proze is?\n",
        "4. A  little bill.\n",
        "5. What does Caterpillar say?\n",
        "6. Time to cook.\n",
        "7. Tea time.\n",
        "8. Who is under sentence of execution ?\n",
        "9. What the mock would say ?\n",
        "10. A beautiful dance.\n",
        "11. In the court!\n",
        "12. The king and the jury last decision.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K7fccamCyYBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Top 10 most used verbs in sentences with Alice"
      ],
      "metadata": {
        "id": "XVjcnyYGcLYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Finding the verbs in sentences with Alice"
      ],
      "metadata": {
        "id": "ItlLKE4SwJxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find the most common verbs we need to find the POS tag for each token in each sentence. Hence, instead of directly spliting the story into tokens as in the previous work, here we need to split the data into sentences first, then we find the POS tag for each token in each sentencce."
      ],
      "metadata": {
        "id": "2IzHuV-cubxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the story into sentences\n",
        "sentences = nltk.sent_tokenize(story)"
      ],
      "metadata": {
        "id": "at2tjiGZcTV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the sentences that contains the word (alice) in order to find the verbs in these sentences as required.\n",
        "sentences_with_alice = [sen for sen in sentences if 'alice' in sen.lower()]"
      ],
      "metadata": {
        "id": "Hv6c404SfFSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tagging the sentences\n",
        "sentences_tagged=[] # this list will contain each sentence tagged\n",
        "for i in sentences_with_alice: \n",
        "  # Tokenizing each sentence individually\n",
        "  wordsList = nltk.word_tokenize(i) \n",
        "  # Removing punctuation from each word\n",
        "  stripped_2 = [ w.translate(table) for w in wordsList]\n",
        "  # Removing stop words and non alphabtecis words \n",
        "  wordsList = [w.translate(table) for w in stripped_2 if not w in stop_words and w.isalpha()]\n",
        "  # Using a Tagger. Which is part-of-speech tagger or POS-tagger. \n",
        "  tagged = nltk.pos_tag(wordsList) \n",
        "  sentences_tagged.append(tagged) "
      ],
      "metadata": {
        "id": "9SNXNFdBdosN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the verbs in the tagged sentences\n",
        "# All the tags that are related to verb: VB, VBD, VBG, VBN, VBP, VBZ\n",
        "verb_tags=['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "verbs =[]\n",
        "# keeping only the verbs \n",
        "for sen in sentences_tagged:\n",
        "  verbs.append([verb for verb,tag in sen if tag in verb_tags])\n",
        "all_verbs=[j for i in verbs for j in i] # This variable contains all the verbs in one list\n",
        "#Finding the lemma for each verb in order to find the most frequent verb\n",
        "verbs_lemmas = [wn.lemmatize(word,'v') for word in all_verbs] "
      ],
      "metadata": {
        "id": "WjE-Um6ukEzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Finding the 10 most frequent verbs"
      ],
      "metadata": {
        "id": "-IaemgQjwV3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Creating a dataframe out of the list of verbs_lemmas\n",
        "df_all_words = pd.DataFrame(verbs_lemmas, columns=['verb'])\n",
        "# Grouping the verbs and find each verb frequency\n",
        "df_all_words['counts'] = df_all_words.groupby(['verb'])['verb'].transform('count')\n",
        "# Sorting the verbs by their frequencies\n",
        "df_all_words = df_all_words.sort_values(by=['counts', 'verb'], ascending=[False, True]).reset_index()\n",
        "# Finding most 10 frequent verbs\n",
        "df_words = df_all_words.groupby('verb').first().sort_values(by='counts', ascending=False).reset_index()\n",
        "print(\"Most 10 frequent verbs:\")\n",
        "print(df_words.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLJrc0T9iIaT",
        "outputId": "c861c465-a9aa-4677-d3b0-634eae4cd064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most 10 frequent verbs:\n",
            "    verb  index  counts\n",
            "0    say     14     295\n",
            "1     go     27      91\n",
            "2  think     13      64\n",
            "3    get      1      61\n",
            "4   look     20      49\n",
            "5   know     92      46\n",
            "6  begin      0      42\n",
            "7    see     23      38\n",
            "8   come     76      33\n",
            "9   make      7      33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As a result**, most of the time Alice saies. Usually she goes, thinks, and gets. less frequent she looks, knows, begins, sees, comes and makes."
      ],
      "metadata": {
        "id": "HlIbctpiw59n"
      }
    }
  ]
}